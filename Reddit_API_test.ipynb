{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the list of political subReddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction: https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.reddit.com/r/redditlists/comments/josdr/list_of_political_subreddits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>List of political subreddits. : redditlists</title>\n",
      "World News\n",
      "http://www.reddit.com/r/worldnews\n",
      "News\n",
      "http://www.reddit.com/r/news\n",
      "World Politics\n",
      "http://www.reddit.com/r/worldpolitics\n",
      "World Events\n",
      "http://www.reddit.com/r/Worldevents\n",
      "Business\n",
      "http://reddit.com/r/business\n",
      "Economics\n",
      "http://reddit.com/r/Economics\n",
      "Environment\n",
      "http://reddit.com/r/environment\n",
      "Energy\n",
      "http://www.reddit.com/r/energy\n",
      "Law\n",
      "http://reddit.com/r/law/\n",
      "Education\n",
      "http://www.reddit.com/r/education/\n",
      "Government\n",
      "http://www.reddit.com/r/government/\n",
      "History\n",
      "http://www.reddit.com/r/history/\n",
      "Politics PDF's\n",
      "http://www.reddit.com/r/PoliticsPDFs/\n",
      "Wikileaks\n",
      "http://www.reddit.com/r/WikiLeaks/\n",
      "SOPA\n",
      "http://www.reddit.com/r/SOPA/\n",
      "News Porn\n",
      "http://www.reddit.com/r/NewsPorn\n",
      "World News 2\n",
      "http://www.reddit.com/r/worldnews2/\n",
      "Anarchist News\n",
      "http://www.reddit.com/r/AnarchistNews/\n",
      "Republic of Politics\n",
      "http://www.reddit.com/r/republicofpolitics\n",
      "LGBT News\n",
      "http://www.reddit.com/r/LGBTnews/\n",
      "Politics 2\n",
      "http://www.reddit.com/r/politics2/\n",
      "Economics 2\n",
      "http://www.reddit.com/r/economics2/\n",
      "Environment 2\n",
      "http://www.reddit.com/r/environment2/\n",
      "Politics\n",
      "http://www.reddit.com/r/politics\n",
      "US Politics\n",
      "http://www.reddit.com/r/uspolitics\n",
      "American Politics\n",
      "http://reddit.com/r/AmericanPolitics\n",
      "Canada\n",
      "http://www.reddit.com/r/canada\n",
      "American Government\n",
      "http://reddit.com/r/AmericanGovernment\n",
      "UK Politics\n",
      "http://reddit.com/r/ukpolitics\n",
      "Euro\n",
      "http://www.reddit.com/r/euro\n",
      "Palestine\n",
      "http://www.reddit.com/r/Palestine/\n",
      "EU Politics\n",
      "http://www.reddit.com/r/eupolitics\n",
      "Middle East News\n",
      "http://www.reddit.com/r/MiddleEastNews\n",
      "Israel\n",
      "http://www.reddit.com/r/Israel/\n",
      "India\n",
      "http://www.reddit.com/r/india\n",
      "Pakistan\n",
      "http://www.reddit.com/r/pakistan\n",
      "Cascadia\n",
      "http://www.reddit.com/r/Cascadia\n",
      "Iran\n",
      "http://www.reddit.com/r/iran\n",
      "Wisconsin\n",
      "http://www.reddit.com/r/wisconsinpolitics\n",
      "Libertarian\n",
      "http://reddit.com/r/Libertarian\n",
      "Anarchism\n",
      "http://reddit.com/r/Anarchism\n",
      "Socialism\n",
      "http://reddit.com/r/socialism\n",
      "Progressive\n",
      "http://reddit.com/r/progressive\n",
      "Conservative\n",
      "http://reddit.com/r/Conservative\n",
      "American Pirate Party\n",
      "http://reddit.com/r/americanpirateparty\n",
      "Democrats\n",
      "http://reddit.com/r/democrats\n",
      "Liberal\n",
      "http://reddit.com/r/Liberal\n",
      "New Right\n",
      "http://reddit.com/r/new_right\n",
      "Republican\n",
      "http://reddit.com/r/Republican\n",
      "Egalitarian\n",
      "http://www.reddit.com/r/egalitarian\n",
      "Democratic Socialism\n",
      "http://www.reddit.com/r/demsocialist\n",
      "Libertarian Left\n",
      "http://www.reddit.com/r/LibertarianLeft\n",
      "Liberty\n",
      "http://www.reddit.com/r/Liberty\n",
      "Anarcho Capitalism\n",
      "http://www.reddit.com/r/Anarcho_Capitalism\n",
      "All the Left\n",
      "http://www.reddit.com/r/alltheleft\n",
      "Neo-Progressives\n",
      "http://www.reddit.com/r/neoprogs\n",
      "Tea Party\n",
      "http://www.reddit.com/r/tea_party\n",
      "Voluntarism\n",
      "http://www.reddit.com/r/voluntarism/\n",
      "Labor\n",
      "http://reddit.com/r/labor\n",
      "Black Flag\n",
      "http://www.reddit.com/r/blackflag\n",
      "Green Party\n",
      "http://www.reddit.com/r/GreenParty\n",
      "Democracy\n",
      "http://www.reddit.com/r/democracy\n",
      "International Workers of the World\n",
      "http://www.reddit.com/r/IWW\n",
      "Pirate Party\n",
      "http://www.reddit.com/r/PirateParty\n",
      "Marxism\n",
      "http://www.reddit.com/r/Marxism\n",
      "Piratenpartei Deutschland\n",
      "http://www.reddit.com/r/piratenpartei\n",
      "Objectivism\n",
      "http://www.reddit.com/r/Objectivism\n",
      "Libertarian Socialism\n",
      "http://www.reddit.com/r/LibertarianSocialism\n",
      "People's Party\n",
      "http://www.reddit.com/r/peoplesparty\n",
      "Capitalism\n",
      "http://www.reddit.com/r/Capitalism\n",
      "Anarchist\n",
      "http://www.reddit.com/r/Anarchist\n",
      "Feminisms\n",
      "http://www.reddit.com/r/feminisms\n",
      "Republicans\n",
      "http://www.reddit.com/r/republicans\n",
      "Egalitarianism\n",
      "http://www.reddit.com/r/Egalitarianism\n",
      "AnarchaFeminism\n",
      "http://www.reddit.com/r/anarchafeminism\n",
      "Communist\n",
      "http://www.reddit.com/r/Communist\n",
      "Social Democracy\n",
      "http://www.reddit.com/r/socialdemocracy\n",
      "Post Left Anarchism\n",
      "http://www.reddit.com/r/Postleftanarchism\n",
      "Radical Feminism\n",
      "http://www.reddit.com/r/RadicalFeminism/\n",
      "Anarcho Pacifism\n",
      "http://www.reddit.com/r/AnarchoPacifism\n",
      "Conservatives\n",
      "http://www.reddit.com/r/conservatives/\n",
      "Republicanism\n",
      "http://reddit.com/r/republicanism\n",
      "Free Thought\n",
      "http://www.reddit.com/r/Freethought\n",
      "Food for Thought\n",
      "http://www.reddit.com/r/Foodforthought\n",
      "State of the Union\n",
      "http://www.reddit.com/r/StateOfTheUnion/\n",
      "Moderate Politics\n",
      "http://reddit.com/r/moderatepolitics\n",
      "Political Discussion\n",
      "http://www.reddit.com/r/PoliticalDiscussion\n",
      "Equality\n",
      "http://www.reddit.com/r/equality\n",
      "Cultural Studies\n",
      "http://reddit.com/r/culturalstudies\n",
      "Political Humor\n",
      "http://www.reddit.com/r/politicalhumor\n",
      "Propaganda Posters\n",
      "http://reddit.com/r/propagandaposters\n",
      "Social Science\n",
      "http://www.reddit.com/r/SocialScience/\n",
      "Political Philosophy\n",
      "http://www.reddit.com/r/PoliticalPhilosophy\n",
      "Media\n",
      "http://www.reddit.com/r/media\n",
      "Culture\n",
      "http://www.reddit.com/r/culture\n",
      "Racism\n",
      "http://www.reddit.com/r/racism\n",
      "Corruption\n",
      "http://www.reddit.com/r/corruption\n",
      "Intellectual Property Rights\n",
      "http://reddit.com/r/ipr\n",
      "Noam Chomsky\n",
      "http://www.reddit.com/r/chomsky\n",
      "Propaganda\n",
      "http://reddit.com/r/propaganda\n",
      "Peter Schiff\n",
      "http://www.reddit.com/r/PeterSchiff\n",
      "Voting Theory\n",
      "http://www.reddit.com/r/votingtheory\n",
      "Religion In America\n",
      "http://www.reddit.com/r/ReligionInAmerica/\n",
      "Economics Papers\n",
      "http://www.reddit.com/r/EconPapers\n",
      "Debate\n",
      "http://www.reddit.com/r/debate\n",
      "Food Sovereignty\n",
      "http://www.reddit.com/r/FoodSovereignty/\n",
      "Environmental Policy\n",
      "http://www.reddit.com/r/environmental_policy\n",
      "LGBT\n",
      "http://www.reddit.com/r/lgbt/\n",
      "Men's Rights\n",
      "http://www.reddit.com/r/MensRights\n",
      "Collapse\n",
      "http://www.reddit.com/r/collapse\n",
      "Operation Grab Ass\n",
      "http://www.reddit.com/r/OperationGrabAss\n",
      "Hack Bloc\n",
      "http://www.reddit.com/r/HackBloc\n",
      "RPAC - The Open Source Democracy Foundation\n",
      "http://www.reddit.com/r/rpac\n",
      "Bad Cop No Donut\n",
      "http://www.reddit.com/r/Bad_Cop_No_Donut/\n",
      "Anti-Consumption\n",
      "http://www.reddit.com/r/Anticonsumption\n",
      "Permaculture\n",
      "http://www.reddit.com/r/Permaculture/\n",
      "Food 2\n",
      "http://www.reddit.com/r/food2\n",
      "Anonymous\n",
      "http://reddit.com/r/anonymous\n",
      "Censorship\n",
      "http://www.reddit.com/r/censorship\n",
      "Feminism\n",
      "http://www.reddit.com/r/feminism\n",
      "Sunlight\n",
      "http://www.reddit.com/r/Sunlight\n",
      "Privacy\n",
      "http://www.reddit.com/r/privacy/\n",
      "Occupy Wall Street\n",
      "http://www.reddit.com/r/occupywallstreet\n",
      "Resilient Communities\n",
      "http://www.reddit.com/r/resilientcommunities\n",
      "Change Now\n",
      "http://www.reddit.com/r/ChangeNow\n",
      "Reddit Political Activism\n",
      "http://www.reddit.com/r/rpa\n",
      "Gender Egaliatarian\n",
      "http://www.reddit.com/r/GenderEgalitarian\n",
      "Activism\n",
      "http://www.reddit.com/r/activism\n",
      "Revolution\n",
      "http://www.reddit.com/r/revolution\n",
      "Nazi Hunting\n",
      "http://www.reddit.com/r/NaziHunting\n",
      "Prison Reform\n",
      "http://www.reddit.com/r/prisonreform\n",
      "TSA\n",
      "http://www.reddit.com/r/tsa\n",
      "Election Reform\n",
      "http://www.reddit.com/r/electionreform\n",
      "Troubled Teens\n",
      "http://www.reddit.com/r/troubledteens\n",
      "First Amendment\n",
      "http://www.reddit.com/r/firstamendment\n",
      "Sensible Washington\n",
      "http://www.reddit.com/r/sensiblewashington/\n",
      "The War on Drugs\n",
      "http://www.reddit.com/r/Thewarondrugs/\n",
      "Union\n",
      "http://www.reddit.com/r/union/\n",
      "Good Cop Free Donut\n",
      "http://www.reddit.com/r/Good_Cop_Free_Donut\n",
      "Strike Action\n",
      "http://www.reddit.com/r/strikeaction/\n",
      "Youth Rights\n",
      "http://www.reddit.com/r/YouthRights\n",
      "Phx Class War Council\n",
      "http://www.reddit.com/r/PhxClassWarCouncil\n",
      "Human Rights\n",
      "http://www.reddit.com/r/humanrights\n",
      "CPAR\n",
      "http://www.reddit.com/r/CPAR/\n",
      "White Rights\n",
      "http://www.reddit.com/r/whiterights\n",
      "Obama\n",
      "http://reddit.com/r/obama\n",
      "Ron Paul\n",
      "http://reddit.com/r/ronpaul\n",
      "Kucinich\n",
      "http://www.reddit.com/r/Kucinich\n",
      "Palin Problem\n",
      "http://reddit.com/r/PalinProblem\n",
      "Gary Johnson\n",
      "http://www.reddit.com/r/GaryJohnson\n",
      "Bachmann 2012\n",
      "http://www.reddit.com/r/Bachmann2012/\n",
      "United States Presidential Election 2012\n",
      "http://www.reddit.com/r/USPE12\n",
      "2012 Elections\n",
      "http://www.reddit.com/r/2012Elections\n",
      "Campaigns\n",
      "http://www.reddit.com/r/Campaigns\n",
      "Perry 2012\n",
      "http://www.reddit.com/r/perry2012\n",
      "Huntsman\n",
      "http://www.reddit.com/r/huntsman\n",
      "Huntsman 2012\n",
      "http://www.reddit.com/r/huntsman2012\n",
      "Newt 2012\n",
      "http://www.reddit.com/r/newt2012\n",
      "Romney\n",
      "http://www.reddit.com/r/romney\n",
      "Black Ops\n",
      "http://www.reddit.com/r/BlackOps\n",
      "Intelligence\n",
      "http://www.reddit.com/r/Intelligence\n",
      "MidEast Peace\n",
      "http://reddit.com/r/MideastPeace\n",
      "Endless War\n",
      "http://reddit.com/r/EndlessWar\n",
      "Antiwar\n",
      "http://reddit.com/r/antiwar\n",
      "War\n",
      "http://www.reddit.com/r/war\n",
      "Peace\n",
      "http://www.reddit.com/r/peace\n",
      "Afghanistan\n",
      "http://www.reddit.com/r/afghanistan/\n",
      "Libya\n",
      "http://www.reddit.com/r/Libya\n",
      "Conspiracy\n",
      "http://www.reddit.com/r/conspiracy\n",
      "9/11 Truth\n",
      "http://www.reddit.com/r/911truth\n",
      "Climate Skeptics\n",
      "http://www.reddit.com/r/climateskeptics\n",
      "Info Graffiti\n",
      "http://www.reddit.com/r/infograffiti\n",
      "Conspiracy Hub\n",
      "http://www.reddit.com/r/conspiracyhub\n",
      "Redditors for 9/11 Truth\n",
      "http://www.reddit.com/r/redditorsfor911truth\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "webpage = urlopen(req).read()\n",
    "page_soup = soup(webpage, \"html.parser\")\n",
    "title = page_soup.find(\"title\")\n",
    "print(title)\n",
    "containers = page_soup.findAll('a',class_=\"_3t5uN8xUmg0TOwRCOGQEcU\")\n",
    "\n",
    "names=[]\n",
    "urls=[]\n",
    "pairs=[]\n",
    "\n",
    "for container in containers[:174]:\n",
    "    names.append(container.text)\n",
    "    urls.append(container['href'])\n",
    "    pairs.append((container.text,container['href']))\n",
    "    print(container.text)\n",
    "    print(container['href'])\n",
    "\n",
    "subreddit=pd.DataFrame(pairs,columns=['name','url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World News</td>\n",
       "      <td>http://www.reddit.com/r/worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td>http://www.reddit.com/r/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World Politics</td>\n",
       "      <td>http://www.reddit.com/r/worldpolitics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World Events</td>\n",
       "      <td>http://www.reddit.com/r/Worldevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>http://reddit.com/r/business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>9/11 Truth</td>\n",
       "      <td>http://www.reddit.com/r/911truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Climate Skeptics</td>\n",
       "      <td>http://www.reddit.com/r/climateskeptics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Info Graffiti</td>\n",
       "      <td>http://www.reddit.com/r/infograffiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Conspiracy Hub</td>\n",
       "      <td>http://www.reddit.com/r/conspiracyhub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Redditors for 9/11 Truth</td>\n",
       "      <td>http://www.reddit.com/r/redditorsfor911truth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name                                           url\n",
       "0                  World News             http://www.reddit.com/r/worldnews\n",
       "1                        News                  http://www.reddit.com/r/news\n",
       "2              World Politics         http://www.reddit.com/r/worldpolitics\n",
       "3                World Events           http://www.reddit.com/r/Worldevents\n",
       "4                    Business                  http://reddit.com/r/business\n",
       "..                        ...                                           ...\n",
       "169                9/11 Truth              http://www.reddit.com/r/911truth\n",
       "170          Climate Skeptics       http://www.reddit.com/r/climateskeptics\n",
       "171             Info Graffiti          http://www.reddit.com/r/infograffiti\n",
       "172            Conspiracy Hub         http://www.reddit.com/r/conspiracyhub\n",
       "173  Redditors for 9/11 Truth  http://www.reddit.com/r/redditorsfor911truth\n",
       "\n",
       "[174 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit.to_csv('Data/subReddit_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Reddit Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('fQ-VABpboAY8iQ', 'v6X0ifFGgMRop3g2TlbV8cyX0LXxOQ')\n",
    "\n",
    "# here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': 'JoyceOoops',\n",
    "        'password': '19960703jyr'}\n",
    "\n",
    "# setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "# send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "\n",
    "# add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "# Add headers=headers to every request\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id = 'fQ-VABpboAY8iQ', \n",
    "                     client_secret = 'v6X0ifFGgMRop3g2TlbV8cyX0LXxOQ', \n",
    "                     user_agent = 'JoyceOoops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "received 403 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1c9d2da9df30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                    \"search_type\":[]}\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtopics_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtopics_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/models/listing/generator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/models/listing/generator.py\u001b[0m in \u001b[0;36m_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# for submission duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objectify_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     def info(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36m_objectify_request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                 \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             )\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBadRequest\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    258\u001b[0m             )\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mForbidden\u001b[0m: received 403 HTTP response"
     ]
    }
   ],
   "source": [
    "for x in range(len(names))[17:]:\n",
    "    \n",
    "    subreddit = reddit.subreddit(urls[x].split('/')[4]) \n",
    "    posts = subreddit.hot(limit=1000)\n",
    "\n",
    "    topics_dict = { \"subreddit\":[],\n",
    "                   \"title\":[],\n",
    "                   \"score\":[], \n",
    "                   \"id\":[], \n",
    "                   \"url\":[], \n",
    "                   \"comms_num\": [], \n",
    "                   \"created\": [], \n",
    "                   \"body\":[],\n",
    "                   \"search_type\":[]}\n",
    "\n",
    "    for submission in posts:\n",
    "        topics_dict[\"subreddit\"].append(names[x])\n",
    "        topics_dict[\"id\"].append(submission.id)\n",
    "        topics_dict[\"title\"].append(submission.title)\n",
    "        topics_dict[\"score\"].append(submission.score)\n",
    "        topics_dict[\"url\"].append(submission.url)\n",
    "        topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "        topics_dict[\"created\"].append(datetime.datetime.fromtimestamp(submission.created))\n",
    "    #datetime.datetime.fromtimestamp(submission.created)\n",
    "        topics_dict[\"body\"].append(submission.selftext)\n",
    "        topics_dict[\"search_type\"].append(\"new\")\n",
    "\n",
    "    topics_data = pd.DataFrame(topics_dict)\n",
    "    topics_data.head()\n",
    "\n",
    "    topics_data.to_csv(\"Data/PRAW/PRAW_\"+names[x]+\".csv\")\n",
    "    print(x,names[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRAW_Business.csv\n",
      "PRAW_World News.csv\n",
      "PRAW_Energy.csv\n",
      "PRAW_World Events.csv\n",
      "PRAW_World News 2.csv\n",
      "PRAW_News.csv\n",
      "PRAW_History.csv\n",
      "PRAW_Economics.csv\n",
      "PRAW_Politics PDF's.csv\n",
      "PRAW_Environment.csv\n",
      "PRAW.csv\n",
      "PRAW_Government.csv\n",
      "PRAW_Law.csv\n",
      "PRAW_Wikileaks.csv\n",
      "PRAW_News Porn.csv\n",
      "PRAW_Education.csv\n",
      "PRAW_World Politics.csv\n",
      "PRAW_SOPA.csv\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "docs_location = \"Data/PRAW/\"\n",
    "\n",
    "for doc in os.listdir(docs_location):\n",
    "    print(doc)\n",
    "    df = pd.read_csv(os.path.join(docs_location + doc))\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>search_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Posts regarding politics</td>\n",
       "      <td>81</td>\n",
       "      <td>kurvl4</td>\n",
       "      <td>https://www.reddit.com/r/business/comments/kur...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-11 01:15:49</td>\n",
       "      <td>Many of you know, we have a strict no-politics...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>USDOT report shows autonomous trucks will only...</td>\n",
       "      <td>271</td>\n",
       "      <td>m8izf6</td>\n",
       "      <td>https://www.traffictechnologytoday.com/news/au...</td>\n",
       "      <td>58</td>\n",
       "      <td>2021-03-19 15:54:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>NFL announces new TV deals, with Amazon gettin...</td>\n",
       "      <td>314</td>\n",
       "      <td>m8ganl</td>\n",
       "      <td>https://www.washingtonpost.com/sports/2021/03/...</td>\n",
       "      <td>67</td>\n",
       "      <td>2021-03-19 13:37:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Ford to hold some F-150s without certain modul...</td>\n",
       "      <td>197</td>\n",
       "      <td>m8j14u</td>\n",
       "      <td>https://www.autonews.com/manufacturing/ford-ho...</td>\n",
       "      <td>39</td>\n",
       "      <td>2021-03-19 15:56:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Lyft says it just had the most riders in a sin...</td>\n",
       "      <td>625</td>\n",
       "      <td>m7zhs6</td>\n",
       "      <td>https://www.cnbc.com/2021/03/18/lyft-says-it-j...</td>\n",
       "      <td>25</td>\n",
       "      <td>2021-03-18 21:09:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                              title  score      id  \\\n",
       "0  Business                           Posts regarding politics     81  kurvl4   \n",
       "1  Business  USDOT report shows autonomous trucks will only...    271  m8izf6   \n",
       "2  Business  NFL announces new TV deals, with Amazon gettin...    314  m8ganl   \n",
       "3  Business  Ford to hold some F-150s without certain modul...    197  m8j14u   \n",
       "4  Business  Lyft says it just had the most riders in a sin...    625  m7zhs6   \n",
       "\n",
       "                                                 url  comms_num  \\\n",
       "0  https://www.reddit.com/r/business/comments/kur...          0   \n",
       "1  https://www.traffictechnologytoday.com/news/au...         58   \n",
       "2  https://www.washingtonpost.com/sports/2021/03/...         67   \n",
       "3  https://www.autonews.com/manufacturing/ford-ho...         39   \n",
       "4  https://www.cnbc.com/2021/03/18/lyft-says-it-j...         25   \n",
       "\n",
       "               created                                               body  \\\n",
       "0  2021-01-11 01:15:49  Many of you know, we have a strict no-politics...   \n",
       "1  2021-03-19 15:54:04                                                NaN   \n",
       "2  2021-03-19 13:37:02                                                NaN   \n",
       "3  2021-03-19 15:56:21                                                NaN   \n",
       "4  2021-03-18 21:09:32                                                NaN   \n",
       "\n",
       "  search_type  \n",
       "0         new  \n",
       "1         new  \n",
       "2         new  \n",
       "3         new  \n",
       "4         new  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat(dfs, sort=False)\n",
    "df=df.reset_index(drop=True)\n",
    "df=df.drop(columns=['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/PRAW_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>search_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Posts regarding politics</td>\n",
       "      <td>81</td>\n",
       "      <td>kurvl4</td>\n",
       "      <td>https://www.reddit.com/r/business/comments/kur...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-11 01:15:49</td>\n",
       "      <td>Many of you know, we have a strict no-politics...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>USDOT report shows autonomous trucks will only...</td>\n",
       "      <td>271</td>\n",
       "      <td>m8izf6</td>\n",
       "      <td>https://www.traffictechnologytoday.com/news/au...</td>\n",
       "      <td>58</td>\n",
       "      <td>2021-03-19 15:54:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>NFL announces new TV deals, with Amazon gettin...</td>\n",
       "      <td>314</td>\n",
       "      <td>m8ganl</td>\n",
       "      <td>https://www.washingtonpost.com/sports/2021/03/...</td>\n",
       "      <td>67</td>\n",
       "      <td>2021-03-19 13:37:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Ford to hold some F-150s without certain modul...</td>\n",
       "      <td>197</td>\n",
       "      <td>m8j14u</td>\n",
       "      <td>https://www.autonews.com/manufacturing/ford-ho...</td>\n",
       "      <td>39</td>\n",
       "      <td>2021-03-19 15:56:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Lyft says it just had the most riders in a sin...</td>\n",
       "      <td>625</td>\n",
       "      <td>m7zhs6</td>\n",
       "      <td>https://www.cnbc.com/2021/03/18/lyft-says-it-j...</td>\n",
       "      <td>25</td>\n",
       "      <td>2021-03-18 21:09:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13628</th>\n",
       "      <td>SOPA</td>\n",
       "      <td>Make Congress embarrassed of their support for...</td>\n",
       "      <td>3</td>\n",
       "      <td>sev7u</td>\n",
       "      <td>http://torrentfreak.com/hey-congress-i-visited...</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-04-17 22:30:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13629</th>\n",
       "      <td>SOPA</td>\n",
       "      <td>Stop Cyber Spying Week Launches Protest</td>\n",
       "      <td>12</td>\n",
       "      <td>se55b</td>\n",
       "      <td>http://www.infozine.com/news/stories/op/storie...</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-04-17 14:16:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13630</th>\n",
       "      <td>SOPA</td>\n",
       "      <td>Netflix forms PAC, positions itself to support...</td>\n",
       "      <td>441</td>\n",
       "      <td>rzcrn</td>\n",
       "      <td>http://www.politico.com/news/stories/0412/7492...</td>\n",
       "      <td>113</td>\n",
       "      <td>2012-04-08 17:27:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13631</th>\n",
       "      <td>SOPA</td>\n",
       "      <td>Reddit, there's a really simple way that we ca...</td>\n",
       "      <td>4</td>\n",
       "      <td>pls8g</td>\n",
       "      <td>https://www.reddit.com/r/SOPA/comments/pls8g/r...</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-02-12 06:20:24</td>\n",
       "      <td>I've been running this all day, but I think it...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13632</th>\n",
       "      <td>SOPA</td>\n",
       "      <td>So, can anybody tell me why the DMCA is no lon...</td>\n",
       "      <td>8</td>\n",
       "      <td>negnt</td>\n",
       "      <td>https://www.reddit.com/r/SOPA/comments/negnt/s...</td>\n",
       "      <td>10</td>\n",
       "      <td>2011-12-15 23:58:44</td>\n",
       "      <td>I've personally had to study the DMCA (Digital...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13633 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                              title  score  \\\n",
       "0      Business                           Posts regarding politics     81   \n",
       "1      Business  USDOT report shows autonomous trucks will only...    271   \n",
       "2      Business  NFL announces new TV deals, with Amazon gettin...    314   \n",
       "3      Business  Ford to hold some F-150s without certain modul...    197   \n",
       "4      Business  Lyft says it just had the most riders in a sin...    625   \n",
       "...         ...                                                ...    ...   \n",
       "13628      SOPA  Make Congress embarrassed of their support for...      3   \n",
       "13629      SOPA            Stop Cyber Spying Week Launches Protest     12   \n",
       "13630      SOPA  Netflix forms PAC, positions itself to support...    441   \n",
       "13631      SOPA  Reddit, there's a really simple way that we ca...      4   \n",
       "13632      SOPA  So, can anybody tell me why the DMCA is no lon...      8   \n",
       "\n",
       "           id                                                url  comms_num  \\\n",
       "0      kurvl4  https://www.reddit.com/r/business/comments/kur...          0   \n",
       "1      m8izf6  https://www.traffictechnologytoday.com/news/au...         58   \n",
       "2      m8ganl  https://www.washingtonpost.com/sports/2021/03/...         67   \n",
       "3      m8j14u  https://www.autonews.com/manufacturing/ford-ho...         39   \n",
       "4      m7zhs6  https://www.cnbc.com/2021/03/18/lyft-says-it-j...         25   \n",
       "...       ...                                                ...        ...   \n",
       "13628   sev7u  http://torrentfreak.com/hey-congress-i-visited...          0   \n",
       "13629   se55b  http://www.infozine.com/news/stories/op/storie...          0   \n",
       "13630   rzcrn  http://www.politico.com/news/stories/0412/7492...        113   \n",
       "13631   pls8g  https://www.reddit.com/r/SOPA/comments/pls8g/r...          0   \n",
       "13632   negnt  https://www.reddit.com/r/SOPA/comments/negnt/s...         10   \n",
       "\n",
       "                   created                                               body  \\\n",
       "0      2021-01-11 01:15:49  Many of you know, we have a strict no-politics...   \n",
       "1      2021-03-19 15:54:04                                                NaN   \n",
       "2      2021-03-19 13:37:02                                                NaN   \n",
       "3      2021-03-19 15:56:21                                                NaN   \n",
       "4      2021-03-18 21:09:32                                                NaN   \n",
       "...                    ...                                                ...   \n",
       "13628  2012-04-17 22:30:35                                                NaN   \n",
       "13629  2012-04-17 14:16:29                                                NaN   \n",
       "13630  2012-04-08 17:27:52                                                NaN   \n",
       "13631  2012-02-12 06:20:24  I've been running this all day, but I think it...   \n",
       "13632  2011-12-15 23:58:44  I've personally had to study the DMCA (Digital...   \n",
       "\n",
       "      search_type  \n",
       "0             new  \n",
       "1             new  \n",
       "2             new  \n",
       "3             new  \n",
       "4             new  \n",
       "...           ...  \n",
       "13628         new  \n",
       "13629         new  \n",
       "13630         new  \n",
       "13631         new  \n",
       "13632         new  \n",
       "\n",
       "[13633 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "entities=[]\n",
    "\n",
    "for x in df['title']:\n",
    "    doc = nlp(x)\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.text,ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_lists=[]\n",
    "\n",
    "for r in range(len(df.title)):\n",
    "    doc = nlp(df.title[r])\n",
    "    for ent in doc.ents:\n",
    "        NER_lists.append((ent.text, ent.start_char, ent.end_char, ent.label_,df.title[r],df.id[r],df.subreddit[r]))\n",
    "\n",
    "NER=pd.DataFrame(NER_lists,columns=['TEXT','START','END','LABEL','Sentence','Article','subreddit'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER.to_csv('Data/NER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Reddit_SampleData2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER=pd.read_csv('Data/NER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>USDOT</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>ORG</td>\n",
       "      <td>USDOT report shows autonomous trucks will only...</td>\n",
       "      <td>m8izf6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NFL</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NFL announces new TV deals, with Amazon gettin...</td>\n",
       "      <td>m8ganl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NFL announces new TV deals, with Amazon gettin...</td>\n",
       "      <td>m8ganl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Thursdays</td>\n",
       "      <td>48</td>\n",
       "      <td>57</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NFL announces new TV deals, with Amazon gettin...</td>\n",
       "      <td>m8ganl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NFL announces new TV deals, with Amazon gettin...</td>\n",
       "      <td>m8ganl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23729</th>\n",
       "      <td>23729</td>\n",
       "      <td>a week</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>DATE</td>\n",
       "      <td>Several groups Monday launched a week of prote...</td>\n",
       "      <td>se36z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23730</th>\n",
       "      <td>23730</td>\n",
       "      <td>Congress</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Make Congress embarrassed of their support for...</td>\n",
       "      <td>sev7u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23731</th>\n",
       "      <td>23731</td>\n",
       "      <td>CISPA</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Make Congress embarrassed of their support for...</td>\n",
       "      <td>sev7u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23732</th>\n",
       "      <td>23732</td>\n",
       "      <td>Iran</td>\n",
       "      <td>53</td>\n",
       "      <td>57</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Reddit, there's a really simple way that we ca...</td>\n",
       "      <td>pls8g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23733</th>\n",
       "      <td>23733</td>\n",
       "      <td>DMCA</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>ORG</td>\n",
       "      <td>So, can anybody tell me why the DMCA is no lon...</td>\n",
       "      <td>negnt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23734 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       TEXT  START  END     LABEL  \\\n",
       "0               0      USDOT      0    5       ORG   \n",
       "1               1        NFL      0    3       ORG   \n",
       "2               2     Amazon     33   39       ORG   \n",
       "3               3  Thursdays     48   57      DATE   \n",
       "4               4         17     64   66  CARDINAL   \n",
       "...           ...        ...    ...  ...       ...   \n",
       "23729       23729     a week     31   37      DATE   \n",
       "23730       23730   Congress      5   13       ORG   \n",
       "23731       23731      CISPA     47   52       ORG   \n",
       "23732       23732       Iran     53   57       GPE   \n",
       "23733       23733       DMCA     32   36       ORG   \n",
       "\n",
       "                                                Sentence Article  \n",
       "0      USDOT report shows autonomous trucks will only...  m8izf6  \n",
       "1      NFL announces new TV deals, with Amazon gettin...  m8ganl  \n",
       "2      NFL announces new TV deals, with Amazon gettin...  m8ganl  \n",
       "3      NFL announces new TV deals, with Amazon gettin...  m8ganl  \n",
       "4      NFL announces new TV deals, with Amazon gettin...  m8ganl  \n",
       "...                                                  ...     ...  \n",
       "23729  Several groups Monday launched a week of prote...   se36z  \n",
       "23730  Make Congress embarrassed of their support for...   sev7u  \n",
       "23731  Make Congress embarrassed of their support for...   sev7u  \n",
       "23732  Reddit, there's a really simple way that we ca...   pls8g  \n",
       "23733  So, can anybody tell me why the DMCA is no lon...   negnt  \n",
       "\n",
       "[23734 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Business': 32,\n",
       "         'World News': 211,\n",
       "         'Energy': 174,\n",
       "         'World Events': 146,\n",
       "         'World News 2': 267,\n",
       "         'News': 75,\n",
       "         'History': 178,\n",
       "         'Economics': 135,\n",
       "         \"Politics PDF's\": 119,\n",
       "         'Environment': 121,\n",
       "         'IWW': 192,\n",
       "         'Government': 180,\n",
       "         'Law': 454,\n",
       "         'Wikileaks': 411,\n",
       "         'News Porn': 334,\n",
       "         'Education': 19,\n",
       "         'World Politics': 155,\n",
       "         'SOPA': 223})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(NER[NER['LABEL']=='PERSON'].subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Business': 234,\n",
       "         'World News': 1721,\n",
       "         'Energy': 1776,\n",
       "         'World Events': 1304,\n",
       "         'World News 2': 1988,\n",
       "         'News': 683,\n",
       "         'History': 1371,\n",
       "         'Economics': 1047,\n",
       "         \"Politics PDF's\": 800,\n",
       "         'Environment': 1248,\n",
       "         'IWW': 1243,\n",
       "         'Government': 1465,\n",
       "         'Law': 1735,\n",
       "         'Wikileaks': 1650,\n",
       "         'News Porn': 3026,\n",
       "         'Education': 225,\n",
       "         'World Politics': 740,\n",
       "         'SOPA': 1478})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(NER.subreddit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
